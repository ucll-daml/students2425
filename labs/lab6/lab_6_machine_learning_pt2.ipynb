{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Logo_UCLL_ENG_RGB.png\" style=\"background-color:white;\" />\n",
    "\n",
    "# Data Analytics & Machine Learning\n",
    "\n",
    "Lecturers: Aimée Lynn Backiel, Kenric Borgelioen, and Daan Nijs\n",
    "\n",
    "Academic year 2024-2025\n",
    "\n",
    "## Lab 6: Machine learning, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. Recap of previous weeks\n",
    "2. Overfitting and underfitting\n",
    "3. Automating machine learning pipelines with sci-kit learn\n",
    "4. Model evaluation \n",
    "5. parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last lecture(s)\n",
    "\n",
    "#### Lab 1\n",
    "\n",
    "1. We ensured we had a valid Python installation.\n",
    "2. We learnt what a virtual environment is:\n",
    "   * Isolated Python executable and packages.\n",
    "   * We created a virtual environment.\n",
    "3. Absolute path vs relative path recap.\n",
    "4. Recap of data structures in Python\n",
    "\n",
    "#### Lab 2\n",
    "1. Installed Pandas\n",
    "2. Learnt how to read data\n",
    "3. Learnt how to calculate mean, mode, median etc.\n",
    "4. Basic exploration of the 4 variables\n",
    "\n",
    "#### Lab 3\n",
    "1. Wrapped up computing summary statistics (mean, median, mode, ...)\n",
    "2. Learnt how to deal with outliers \n",
    "3. Focused on exploration of dat\n",
    "\n",
    "#### Lab 4\n",
    "1. Univariate data visualization using Matplotlib\n",
    "   1. Figures and axes\n",
    "   2. Histograms\n",
    "   3. Box plots\n",
    "   4. Bar charts\n",
    "2. Multivariate data visualization using Seaborn\n",
    "   1. Scatter plots\n",
    "   2. Small multiples\n",
    "   3. Color coding\n",
    "\n",
    "#### Lab 5\n",
    "2. Intro to machine learning using scikit-learn\n",
    "   1. Preprocessing\n",
    "      1. One Hot encoding\n",
    "      2. Scaling\n",
    "      3. Outliers\n",
    "   2. Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada Turing Travelogue, or as everyone calls her, Ada just started working part time at her parents travel agency. She has a keen understanding and interest of everything related to applied computer science ranging from server & system management to full stack software development. Through database foundations she already understands how to query data and programming 1 and 2 covered the essentials about the Python programming language. Recently she has just decided to start learning about data analytics & machine learning as well.\n",
    "\n",
    "She uses her skills to connect to the travel agency's database where she finds many, normalized, tables. Ada recalls what she learnt in database foundations and performs all the correct joins. Afterwards she saves the data in the `data/` folder.\n",
    "\n",
    "\n",
    "She finds the following dataset:\n",
    "\n",
    "| Column Name          | Description                                                                                       |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| SalesID              | Unique identifier for each sale.                                                                  |\n",
    "| Age                  | Age of the traveler.                                                                              |\n",
    "| Country              | Country of origin of the traveler.                                                                |\n",
    "| Membership_Status    | Membership level of the traveler in the booking system; could be 'standard', 'silver', or 'gold'. |\n",
    "| Previous_Purchases   | Number of previous bookings made by the traveler.                                                 |\n",
    "| Destination          | Travel destination chosen by the traveler.                                                        |\n",
    "| Stay_length          | Duration of stay at the destination.                                                              |\n",
    "| Guests               | Number of guests traveling (including the primary traveler).                                             |\n",
    "| Travel_month         | Month in which the travel is scheduled.                                                           |\n",
    "| Months_before_travel | Number of months prior to travel that the booking was made.                                       |\n",
    "| Earlybird_discount   | Boolean flag indicating whether the traveler received an early bird discount.                     |\n",
    "| Package_Type         | Type of travel package chosen by the traveler.                                                    |\n",
    "| Cost                 | Calculated cost of the travel package.                                                            |\n",
    "| Margin | The cost (for the traveler) - what the travel agency pays. |\n",
    " | Additional_Services_Cost| The amount of additional services (towels, car rentals, room service, ...) that was bought during the trip. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our challenge\n",
    "\n",
    "Before getting into harder use cases we will start off by predicting the cost of a given stay. Right now Ada's parents do this manually automating this task would already be a big help to their business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.datascience-pm.com/wp-content/uploads/2021/02/CRISP-DM.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also helps to situate our progress within CRISP-DM. We have done the first three steps, as from this lecture we will progress to modeling. As mentioned in the lecture, this is an iterative procedure, as we are doing modeling we need to circle back to both data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning with sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What have we done so far of the image below? What stages have we completed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done train test splitting and we have built a few models on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is when the model doesn't learn general patterns from the data but rather focuses on doing really well on the training data. \n",
    "\n",
    "❗ Typically it means that the performance on the test set will be a lot worse than that on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting is when the model learns a pattern that doesn't significantly capture the details of the training set.\n",
    "\n",
    "❗ Typically it means that the performance on the test set will be similarly (bad) on as that on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://www.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1686825007300.svg\" style=\"background-color:white;width:50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out continuing where we left of last session.\n",
    "\n",
    "❗ While we do this section we will also make a number of methodological mistakes. We do this for two main reasons:\n",
    "* To show how easily \"mistakes\" can be made in machine learning.\n",
    "*  To introduce Pipelines later on which have cleaner syntax and protect you from a lot of things you could do incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # by convention\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import numpy  as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y_true: pd.Series, predictions: np.ndarray, title: str) -> None:\n",
    "    \n",
    "    fig = px.scatter(x=predictions, y=y_true, labels={\"x\": \"predicted\", \"y\": \"actual\"}, title=title)\n",
    "    fig.add_shape(type=\"line\",\n",
    "                x0=-1000, \n",
    "                y0=-1000, \n",
    "                x1=7000, \n",
    "                y1=7000)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_dataset = pd.read_csv(\"data/lab_6_dataset.csv\")\n",
    "X = travel_dataset.drop(columns=\"cost\") # by convention the variables are called (capital) X\n",
    "y = travel_dataset[\"cost\"] # By convention what you want to predict is called (small letter) y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"package_Type\", \"destination\", \"country\"]\n",
    "numeric_columns = [\"guests\", \"age\", \"stay_length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> np.ndarray:\n",
    "\n",
    "    \"\"\"Preprocess the test data by applying a scaling operation on the numeric columns and a one hot encoding on the categorical columns.\n",
    "\n",
    "    data, np.ndarray: the data you wish to transform.\n",
    "    categorical_columns, list[str]: the categorical columns that need to be one-hot encoded.\n",
    "    numeric_columns, list[str]: the numeric columns that need to be scaled.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encoding on the categorical columns\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    ohe.fit(data[cat_columns])\n",
    "    cat_cols_train  = ohe.transform(data[categorical_columns])\n",
    "\n",
    "    # Standard scale the numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    num_cols_train = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "    return np.hstack((cat_cols_train, num_cols_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = preprocess_data(X_train, cat_columns, numeric_columns)\n",
    "X_test_preprocessed = preprocess_data(X_test, cat_columns, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_preprocessed, y_train)\n",
    "predictions_lin_reg = lin_reg.predict(X_train_preprocessed)\n",
    "predictions_test_lin_reg= lin_reg.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_train, predictions_lin_reg, title=\"predicted vs actual for linear regression on the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_test_lin_reg, title=\"predicted versus actual on the test set for linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train_preprocessed, y_train)\n",
    "predictions_dt = decision_tree.predict(X_train_preprocessed)\n",
    "predictions_test_dt = decision_tree.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_train, predictions_dt, title=\"predicted versus actual on the training set for a decision tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, predictions_test_dt, title=\"predicted versus actual on the test set for a decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Which of the two models is overfitting? Can you describe how and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Which of the two models is underfitting? Can you describe how and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pipelines to rectify our mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the preprocessing step is critical to prepare the data for modeling. However, there's a subtle but crucial methodological error in the function shown. The issue lies in the lines where `OneHotEncoder` and `StandardScaler` are fitted:\n",
    "\n",
    "```python\n",
    "def preprocess_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> np.ndarray:\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False) \n",
    "    ohe.fit(data[cat_columns]) # The error occurs here!\n",
    "    cat_cols_train  = ohe.transform(data[categorical_columns])\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols_train = scaler.fit_transform(data[numeric_columns]) # And here!\n",
    "\n",
    "    return np.hstack((cat_cols_train, num_cols_train))  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key mistake in the preprocessing function provided lies in fitting the OneHotEncoder and StandardScaler to the data inside the function. This could lead to a situation where, when preprocessing the test set, the encoders and scalers are fitted again separately with the test set statistics, which is incorrect. The preprocessing steps that \"learn\" from the data, such as encoding categorical variables and scaling numerical variables, should be based solely on the training data to avoid introducing bias from the test set into the model.\n",
    "\n",
    "The proper methodology is to fit the OneHotEncoder and StandardScaler on the training data only. This way, they \"learn\" the categories of the categorical variables and the distribution (mean and standard deviation) of the numerical variables from the training set. Once fitted, these preprocessors should then be applied to the test data, ensuring that the transformation applied is consistent and does not give the model any information about the test set.\n",
    "\n",
    "One potential way to solve it is as follows:\n",
    "\n",
    "```python\n",
    "def fit_preprocessors(train_data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str]) -> (OneHotEncoder, StandardScaler):\n",
    "    # Fit the OneHotEncoder and StandardScaler to the training data\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    scaler = StandardScaler()\n",
    "    ohe.fit(train_data[categorical_columns])\n",
    "    scaler.fit(train_data[numeric_columns])\n",
    "    \n",
    "    # Return the fitted preprocessors\n",
    "    return ohe, scaler\n",
    "\n",
    "def transform_data(data: np.ndarray, categorical_columns: list[str], numeric_columns: list[str], ohe: OneHotEncoder, scaler: StandardScaler) -> np.ndarray:\n",
    "    # Transform data using the already fitted preprocessors\n",
    "    cat_cols = ohe.transform(data[categorical_columns])\n",
    "    num_cols = scaler.transform(data[numeric_columns])\n",
    "    \n",
    "    # Return the transformed data\n",
    "    return np.hstack((cat_cols, num_cols))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, the Pipeline and ColumnTransformer classes offer an idiomatic and streamlined way to chain multiple preprocessing steps and a model into a single workflow. The `Pipeline` class allows you to assemble sequences of transformations and a final model, which simplifies your code and helps prevent common mistakes, such as fitting preprocessing steps to the test data. The `ColumnTransformer` is particularly useful for applying different preprocessing to different columns, such as one-hot encoding for categorical variables and scaling for numerical variables. By combining these tools, you not only reduce the need for manual 'glue' code but also safeguard against the leakage of information from the test set into the training process. We highly recommend you always use this in the scope of this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is quite simple:\n",
    "\n",
    "*  `make_column_transformer` expects multiple tuples. The `transformer` (preprocessing step) is in the first position of the tuple and the columns you apply the transformer (type: `list[str]`) is in the second position.  \n",
    "*  `make_pipeline` similarly expects all objects in sequence, so typically you add your preprocessing first followed by the model you want to apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipe = make_pipeline(preprocessing, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipe.fit(X_train, y_train)\n",
    "predictions_lin_reg_train = lin_reg_pipe.predict(X_train)\n",
    "predictions_lin_reg_test = lin_reg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all!\n",
    "\n",
    "Convince yourself for a second that is a lot simpler than what we were doing previously, we simply use `make_column_transformer` to indicate what preprocessing we want to apply to which column. Afterwards we put our preprocessing in a pipeline with the model we want to use.\n",
    "\n",
    "❗ Once you call `model.fit(X_train, y_train)` it both fits the preprocessing and the model in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines make it easy to make many different models in one go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_pipe = make_pipeline(preprocessing, DecisionTreeRegressor())\n",
    "rf_pipe = make_pipeline(preprocessing, RandomForestRegressor())\n",
    "xgb_pipe = make_pipeline(preprocessing, HistGradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do the following if you want:\n",
    "\n",
    "```python\n",
    "\n",
    "model_name_pair = [(\"random_forest\", RandomForestRegressor()), (\"gradient boosting\", HistGradientBoostingRegressor()), (\"decision tree\", DecisionTreeRegressor())]\n",
    "results = []\n",
    "for pair in model_name_pair:\n",
    "    name, model = pair\n",
    "    pipe = make_pipeline(preprocessing, model)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions_train = pipe.predict(X_train)\n",
    "    predictions_test = pipe.predict(X_test)\n",
    "    result.append([(name, predictions_train, predictions_test)])\n",
    "```\n",
    "\n",
    "The code above would train 3 models in a single for loop. Afterwards you can use a single function to evaluate the results you have obtained from all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we've only *qualitatively* judged the quality of our models by looking at our predicted versus actual plot. We're interested in having a single number that summarizes the the performance of our model. The reason is that investigating graphs doesn't scale well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 0: taking the mean of the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first intuition might be to take the difference of the predictions and the actual values. This is called the **error** or the **residual**. After we have this value we may be tempted to take the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = predictions_lin_reg_test - y_test\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(error, title=\"distribution of the errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Why is the mean of residuals misleading?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ How can we better quantify errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 1: taking the mean of the absolute error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(np.abs(error), title=\"distribution of the absolute errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is more informative as it provides a clearer picture of how much error is present on average in our predictions.\n",
    "\n",
    "By considering the MAE, we obtain a useful summary statistic for model performance that is easy to understand and directly interpretable in terms of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice squaring the errors is more common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attempt 2: Taking the mean of the squared errors (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring the errors, as shown in the equation: $(-2)² = 4 = (2)²$, ensures that all error values are positive. This method has two main benefits:\n",
    "\n",
    "This method has two main advantages:\n",
    "\n",
    "1. Large errors are amplified more than smaller ones, which can be particularly important in cases where larger deviations are less tolerable.\n",
    "2. MSE is a differentiable function, which makes it mathematically convenient for optimization algorithms used in model training. There are other statistical properties that the MSE can leverage.\n",
    "\n",
    "Note: point 2 is only for your information. You don't need to know this at all. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MSE can result in large numbers that are difficult to interpret, we often take the square root to obtain the Root Mean Squared Error (RMSE), which has the same units as the original values:\n",
    "\n",
    "*Note: the root is $\\sqrt(x)$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(np.square(error), title=\"distribution of the square errors of linear regression on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What are downsides of the MSE? There are two, but one is harder to come up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "Our advice:\n",
    "\n",
    "* Compute both the MAE and the RMSE. \n",
    "* The one you should focus on the most is typically the RMSE. \n",
    "* If there are error outliers then the difference between the MAE and the RMSE is likely going to be large. In that case it is typically more interesting to look at the MAE. \n",
    " \n",
    "Typically, RMSE is preferred because it is more sensitive to large errors, which can be critical in applications where such errors are especially problematic, like in autonomous vehicle guidance systems. However, if your model is prone to outliers, or if large errors are less impactful, MAE can be a more relevant metric. Always consider the specific context of your application when choosing your primary evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model evaluation using sci-kit learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sci-kit learn offers all of these functions out of the box. All you need to do is remember their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Use the pipeline approach discussed above and the mean_absolute_error and mean_squared_error for the following models: RandomForestRegressor, HistGradientBoostingRegressor, DecisionTreeRegressor and LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Comment on the behavior of the models. Are they overfitting? Underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first way to improve our model: feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in the previous session we made a number of observations:\n",
    "\n",
    "* If you're visiting the same country as you're from the destination seemed to be cheaper\n",
    "* If you're traveling in approximately the same continent it's also cheaper\n",
    "* There might be the case between age and month.\n",
    "* Maybe we should look at age in groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_to_country = {\n",
    "        \"New York\": \"USA\",\n",
    "        \"Rome\": \"Italy\",\n",
    "        \"Paris\": \"France\",\n",
    "        \"Tokyo\": \"Japan\",\n",
    "        \"Cairo\": \"Egypt\",\n",
    "        \"Sydney\": \"Australia\",\n",
    "        \"Rio\": \"Brazil\",\n",
    "        \"Cape Town\": \"South Africa\",\n",
    "    }\n",
    "country_to_continent = {\n",
    "        \"USA\": \"America\",\n",
    "        \"UK\": \"EMEA\",\n",
    "        \"France\": \"EMEA\",\n",
    "        \"Canada\": \"America\",\n",
    "        \"Australia\": \"Asia\",\n",
    "        \"Germany\": \"EMEA\",\n",
    "        \"Spain\": \"EMEA\",\n",
    "        \"Italy\": \"EMEA\",\n",
    "    }\n",
    "destination_to_continent = {\n",
    "        \"New York\": \"America\",\n",
    "        \"Rome\": \"EMEA\",\n",
    "        \"Paris\": \"EMEA\",\n",
    "        \"Tokyo\": \"Asia\",\n",
    "        \"Cairo\": \"EMEA\",\n",
    "        \"Sydney\": \"Asia\",\n",
    "        \"Rio\": \"America\",\n",
    "        \"Cape Town\": \"Africa\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Use Pandas to make a variables to indicate if they traveled to the same country and then the same continent. \n",
    "\n",
    "##### HINT1: Look at [the map method for Pandas series](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html). \n",
    "\n",
    "##### HINT2: Remember, a series is simply a column so , `df[column]` gives you a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to add these variables there is some friction. They don't fit into our sci-kit learn `Pipeline` workflow nicely. We could add them to our entire dataset before splitting. Adding variables to the entire dataset is not risk-free. Doing so may lead to the methodological error we spoke about previously **data leakage**. In the scope of this course it's fine to use this approach to *add* variables. We'll briefly show you the more principled way, but you don't need to know this for the exam. It involves creating a custom `Transformer` which we can then compose in our pipeline as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CountryMapping(TransformerMixin, BaseEstimator):\n",
    "    country_to_continent: dict[str, str]\n",
    "    destination_to_country: dict[str, str]\n",
    "    destination_to_continent: dict[str, str]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        _X = X.copy() # so we don't change our input data frame\n",
    "        _X['country_match'] = _X['country'] == _X['destination'].map(self.destination_to_country)\n",
    "        _X['continent_match'] = _X['country'].map(self.country_to_continent) == _X['destination'].map(self.destination_to_continent)\n",
    "        return _X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_map = CountryMapping(country_to_continent, destination_to_country, destination_to_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_map.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.jmp.com/en_se/statistics-knowledge-portal/what-is-multiple-regression/mlr-with-interactions/_jcr_content/par/styledcontainer_2069/par/lightbox_3be9/lightboxImage.img.png/1548351208495.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an alternative approach to the problem above using interaction terms. Interaction terms are created by multiplying two variables together, effectively creating a new variable. This is a mathematical way of capturing the 'AND' condition in our data. For example, after one-hot encoding categorical variables like 'Country' and 'Destination', we can generate interaction terms to explore the combined effect of these two features.\n",
    "\n",
    "Suppose we have 'New York', 'Rome', 'Tokyo', and 'Cairo' as categories for 'City' and 'USA', 'Italy', 'Japan', and 'Egypt' for 'Country'. If we create interaction terms for 'City' and 'Country', we end up with additional columns such as 'New York x USA', 'Rome x Italy', and so on. Each of these new columns will have a value of 1 only if both contributing variables (e.g., 'City' is 'New York' AND 'Country' is 'USA') are 1; otherwise, the value will be 0. This new variable thus answers the question: \"Is the traveler from X city AND going to Y country?\"\n",
    "\n",
    "By including interaction terms, we allow our model to consider the combined influence of two variables, which can be particularly insightful when the effect of one variable on the outcome depends on the level of another variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, interaction effects between variables can be encoded using the PolynomialFeatures transformer. Interaction effects are valuable in a model when the relationship between two features can affect the outcome in a way that is not simply additive.\n",
    "\n",
    "For example, consider two binary features, A and B. Individually, they might have a certain effect on the target variable Y. However, when both A and B occur together (i.e., A=1 and B=1), their combined effect on Y could be different from the sum of their individual effects. This is where interaction terms come into play.\n",
    "\n",
    "The PolynomialFeatures transformer can not only generate polynomial features, which are features raised to a power (like $x^{2}$ or $x^{3}$), but also interaction features, which are products of features (like $x_{1} * x_{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by generating interactions between all our numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "numeric_preprocessing = make_pipeline(poly, StandardScaler())\n",
    "\n",
    "preprocessing_poly = make_column_transformer(\n",
    "    (numeric_preprocessing, numeric_columns),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2000/1*LGTAObYYj2-fdBMFLz30rw.jpeg\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning is a technique that involves segmenting a continuous variable into several intervals, or 'bins'. Similar to the way we categorize data when creating histograms, binning transforms a continuous variable into an ordered categorical variable. One hot encoding these bins allows us to introduce non-linear effects into our linear models, which ordinarily would interpret the data as having a constant slope.\n",
    "\n",
    "Take temperature as an example: people generally enjoy mild increases in weather warmth, but there's a threshold beyond which higher temperatures become unpleasant. Binning would let us model this non-linear relationship. Instead of treating temperature as a single continuous predictor with a constant effect, we could divide temperatures into ranges (e.g., 0-10°C, 10-20°C, 20-30°C, etc.) and treat each range as a separate category. By one hot encoding these categories, we enable our model to capture the varying effects of different temperature ranges on people's comfort levels. This approach can reveal more complex patterns in how the predictor variable (in this case, temperature) influences the outcome variable (such as people's reported happiness).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "preprocessing_bins = make_column_transformer(\n",
    "    (StandardScaler(), numeric_columns),\n",
    "    (KBinsDiscretizer(), \"age\"),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocessing_poly_bins = make_column_transformer(\n",
    "    (numeric_preprocessing, numeric_columns),\n",
    "    (KBinsDiscretizer(), \"age\"),\n",
    "    (OneHotEncoder(sparse_output=False), cat_columns),\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best model: cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explore various models and preprocessing techniques, we encounter a dilemma: how do we identify the best model without biasing our selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What is the weakness of the train-test split approach? Think about this before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just one test set might lead us to choose a model that excels on that particular subset of data by sheer luck. What if a different shuffle of the data leads to a different 'best' model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Think about a solution for this before we continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, let's consider a more robust method. Imagine if we could test each model not on one but multiple randomized slices of our data. This is where cross-validation comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's how it works: We divide our training set into smaller sections, say 20% chunks. We train our model on 80% of the data, then validate it on the remaining 20%. We repeat this process five times, each time with a different 20% held out for validation. This technique, known as k-fold cross-validation (with k being the number of chunks or 'folds' we create), allows each model a fair shot at proving itself across the entirety of our data.\n",
    "\n",
    "By averaging the performance across these folds, we obtain a more reliable measure of a model's quality. This thorough approach increases our confidence that we're selecting the best model, not by chance, but by consistent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" style=\"background-color:white\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Sanity check: if we do K-fold cross validation, how many models have we trained. Answer for 2-fold, 3-fold, 5-fold and K-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ What is the downside of K-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, sci-kit learn makes it easy to do cross validation. \n",
    "\n",
    "We supply `cross_val_score` with 3 mandatory parameters:\n",
    "\n",
    "* The machine learning model\n",
    "* The `X_train`\n",
    "* `X_test`\n",
    "  \n",
    "Additionally you can use `cv` to specify how many folds and you can pick a `score` parameter, which is the result that will be reported to you as the performance on each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_reg_cv_results = cross_val_score(lin_reg_pipe, X_train, y_train, cv=5, scoring= \"neg_root_mean_squared_error\")\n",
    "lin_reg_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lin_reg_cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_cv_results = cross_val_score(decision_tree_pipe, X_train, y_train, cv=5, scoring= \"neg_root_mean_squared_error\")\n",
    "decision_tree_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(decision_tree_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Interpret these values. Which model performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Contrast it to the performance seen in the beginning of this notebook of these two models. Think about overfitting, underfitting and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Your turn: experiment with different models and setups. Try out the new pre processing pipelines with a variety of machine learning models and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these together with the previous models. Make new pipelines for each of them.\n",
    "\n",
    "country_map # Add this to the front of your pipeline\n",
    "preprocessing_poly\n",
    "preprocessing_bins\n",
    "preprocessing_poly_bins\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
